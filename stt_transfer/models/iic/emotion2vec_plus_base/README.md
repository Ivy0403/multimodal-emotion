---
frameworks:
- Pytorch
license: Apache License 2.0
tasks:
- emotion-recognition
widgets:
  - enable: true
    version: 1
    task: emotion-recognition
    examples:
      - inputs:
          - data: git://example/test.wav
    inputs:
      - type: audio
        displayType: AudioUploader
        validator:
          max_size: 10M
        name: input
    output:
      displayType: Prediction
      displayValueMapping:
        labels: labels
        scores: scores
    inferencespec:
      cpu: 8
      gpu: 0
      gpu_memory: 0
      memory: 4096
    model_revision: master
    extendsParameters:
      extract_embedding: false
---


<div align="center">
    <h1>
    EMOTION2VEC+
    </h1>
    <p>
    emotion2vec+é€šç”¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»åˆ—åŸºåº§æ¨¡å‹ <br>
    <b>emotion2vec+baseæ¨¡å‹</b>
    </p>
    <p>
    <img src="logo.png" style="width: 200px; height: 200px;">
    </p>
    <p>
    </p>
</div>

# æ¨¡å‹åˆ—è¡¨
GitHub ä»“åº“: [emotion2vec](https://github.com/ddlBoJack/emotion2vec)
|æ¨¡å‹|â­Model Scope|ğŸ¤—Hugging Face|Fine-tuningæ•°æ®é‡ï¼ˆå°æ—¶ï¼‰|
|:---:|:-------------:|:-----------:|:-------------:|
|emotion2vec|[Link](https://www.modelscope.cn/models/iic/emotion2vec_base/summary)|[Link](https://huggingface.co/emotion2vec/emotion2vec_base)|/|
emotion2vec+ seed|[Link](https://modelscope.cn/models/iic/emotion2vec_plus_seed/summary)|[Link](https://huggingface.co/emotion2vec/emotion2vec_plus_seed)|201|
emotion2vec+ base|[Link](https://modelscope.cn/models/iic/emotion2vec_plus_base/summary)|[Link](https://huggingface.co/emotion2vec/emotion2vec_plus_base)|4788|
emotion2vec+ large|[Link](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)|[Link](https://huggingface.co/emotion2vec/emotion2vec_plus_large)|42526|

# æ¨¡å‹ç®€ä»‹
emotion2vec+(emotion2vec_plus)æ˜¯è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«åŸºåº§æ¨¡å‹ï¼Œæˆ‘ä»¬è‡´åŠ›äºæ‰“é€ è¯­éŸ³æƒ…æ„Ÿé¢†åŸŸçš„Whisperï¼Œé€šè¿‡æ•°æ®é©±åŠ¨æ–¹æ³•å°½å¯èƒ½å…‹æœè¯­ç§å’Œå½•åˆ¶åœºæ™¯å¯¹æƒ…æ„Ÿè¯†åˆ«çš„å½±å“ï¼Œè·å¾—é€šç”¨ã€é²æ£’çš„æƒ…æ„Ÿè¯†åˆ«çš„èƒ½åŠ›ã€‚emotion2vec+çš„æ€§èƒ½æ˜¾è‘—è¶…è¿‡å…¶ä»–é«˜ä¸‹è½½é‡çš„huggingfaceå¼€æºæ¨¡å‹ã€‚

![](emotion2vec+radar.png)

æœ¬ç‰ˆæœ¬(emotion2vec_plus_base)ä¸ºä½¿ç”¨ç­›é€‰åçš„å¤§è§„æ¨¡ä¼ªæ ‡æ³¨æ•°æ®è¿›è¡Œfinetuneï¼Œè·å¾—çš„base sizeï¼ˆ~90Mï¼‰æ¨¡å‹ï¼Œç›®å‰æ”¯æŒçš„åˆ†ç±»ä¸ºï¼š
    0: angry
    1: disgusted
    2: fearful
    3: happy
    4: neutral
    5: other
    6: sad
    7: surprised
    8: unknown


# æ•°æ®è¿­ä»£

æˆ‘ä»¬æä¾›ä¸‰ä¸ªç‰ˆæœ¬ï¼Œæ¯ä¸ªç‰ˆæœ¬ç”±å‰ä¸€ä¸ªç‰ˆæœ¬çš„æ•°æ®è¿­ä»£è€Œæ¥ã€‚å¦‚æœéœ€è¦æƒ…æ„Ÿè¡¨å¾æ¨¡å‹ï¼Œå¯ä»¥å‚è€ƒ [emotion2vec](https://github.com/ddlBoJack/emotion2vec)ã€‚
- [emotion2vec+ seed](https://modelscope.cn/models/iic/emotion2vec_plus_seed/summary): ä½¿ç”¨æ¥è‡ª[EmoBox](https://github.com/emo-box/EmoBox)å­¦æœ¯è¯­éŸ³æƒ…æ„Ÿæ•°æ®è¿›è¡Œfinetune
- [emotion2vec+ base](https://modelscope.cn/models/iic/emotion2vec_plus_base/summary): ä½¿ç”¨ç­›é€‰åçš„å¤§è§„æ¨¡ä¼ªæ ‡æ³¨æ•°æ®è¿›è¡Œfinetuneï¼Œè·å¾—çš„base sizeï¼ˆ~90Mï¼‰æ¨¡å‹
- [emotion2vec+ large](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary): ä½¿ç”¨ç­›é€‰åçš„å¤§è§„æ¨¡ä¼ªæ ‡æ³¨æ•°æ®è¿›è¡Œfinetuneï¼Œè·å¾—çš„ large sizeï¼ˆ~300Mï¼‰æ¨¡å‹

è¿­ä»£æµç¨‹å¦‚å›¾æ‰€ç¤ºï¼Œæœ€ç»ˆæˆ‘ä»¬è·å¾—å››ä¸‡å°æ—¶æƒ…æ„Ÿæ•°æ®è¿›è¡Œè®­ç»ƒemotion2vec+ largeæ¨¡å‹ã€‚

# å®‰è£…ç¯å¢ƒ

`pip install -U funasr modelscope`

# ç”¨æ³•

input: 16k Hz çš„è¯­éŸ³

granularity:
- "utterance": æå–æ•´å¥è¯­éŸ³ç‰¹å¾
- "frame": æå–å¸§çº§åˆ«ç‰¹å¾(50 Hz)

extract_embedding: æ˜¯å¦æå–ç‰¹å¾ï¼Œå¦‚æœåªä½¿ç”¨åˆ†ç±»æ¨¡å‹ä¸éœ€è¦ç‰¹å¾è¿™é‡Œè®¾ç½®ä¸ºFalse

## åŸºäºmodelscopeè¿›è¡Œæ¨ç†

```python
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks

inference_pipeline = pipeline(
    task=Tasks.emotion_recognition,
    model="iic/emotion2vec_plus_base")

rec_result = inference_pipeline('https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_zh.wav', granularity="utterance", extract_embedding=False)
print(rec_result)
```


## åŸºäºFunASRè¿›è¡Œæ¨ç†

```python
from funasr import AutoModel

model = AutoModel(model="iic/emotion2vec_plus_base")

wav_file = f"{model.model_path}/example/test.wav"
res = model.generate(wav_file, output_dir="./outputs", granularity="utterance", extract_embedding=False)
print(res)
```
æ³¨ï¼šæ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½

æ”¯æŒè¾“å…¥æ–‡ä»¶åˆ—è¡¨ï¼Œwav.scpï¼ˆkaldié£æ ¼ï¼‰ï¼š
```cat wav.scp
wav_name1 wav_path1.wav
wav_name2 wav_path2.wav
...
```

è¾“å‡ºä¸ºæƒ…æ„Ÿè¡¨å¾å‘é‡ï¼Œä¿å­˜åœ¨`output_dir`ä¸­ï¼Œæ ¼å¼ä¸ºnumpyæ ¼å¼ï¼ˆå¯ä»¥ç”¨np.load()åŠ è½½ï¼‰

# è¯´æ˜

æœ¬ä»“åº“ä¸ºemotion2vecçš„modelscopeç‰ˆæœ¬ï¼Œæ¨¡å‹å‚æ•°å®Œå…¨ä¸€è‡´ã€‚

åŸå§‹ä»“åº“åœ°å€: [https://github.com/ddlBoJack/emotion2vec](https://github.com/ddlBoJack/emotion2vec)

modelscopeç‰ˆæœ¬ä»“åº“ï¼š[https://github.com/alibaba-damo-academy/FunASR](https://github.com/alibaba-damo-academy/FunASR/tree/funasr1.0/examples/industrial_data_pretraining/emotion2vec)

huggingfaceä»“åº“ï¼š[https://huggingface.co/emotion2vec](https://huggingface.co/emotion2vec)

# ç›¸å…³è®ºæ–‡ä»¥åŠå¼•ç”¨ä¿¡æ¯
```BibTeX
@article{ma2023emotion2vec,
  title={emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation},
  author={Ma, Ziyang and Zheng, Zhisheng and Ye, Jiaxin and Li, Jinchao and Gao, Zhifu and Zhang, Shiliang and Chen, Xie},
  journal={arXiv preprint arXiv:2312.15185},
  year={2023}
}
```